{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9193f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import sys  \n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    " \n",
    "if src_path not in sys.path:  \n",
    "    sys.path.append(src_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3baa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from train import train_loop, evaluate\n",
    "from utils import show_metrics, set_random_seeds, save_model, load_model\n",
    "from models import setup_model\n",
    "from quantization import QuantizedModel, fuse_model\n",
    "from datasets import CatCamDataset\n",
    "from pruning import measure_global_sparsity, iterative_pruning_finetuning, remove_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037df789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init parameters\n",
    "random_seed = 42\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "model_dir = \"/content/saved_models\"\n",
    "model_name = \"efficientnet_b0\"\n",
    "model_filename = model_name + \".pt\"\n",
    "pruned_model_filename = model_name + \"_pruned.pt\"\n",
    "quantized_model_filename = model_name + \"_quantized.pt\"\n",
    "onnx_converted_model_filename = model_name + \".onnx\"\n",
    "\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "pruned_model_filepath = os.path.join(model_dir, pruned_model_filename)\n",
    "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
    "onnx_converted_model_filepath = os.path.join(model_dir, onnx_converted_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7568d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44750a01",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7164cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../data\"\n",
    "batch_size = 8\n",
    "\n",
    "train_aug_args = {\n",
    "    \"imgsz\": 224,\n",
    "\n",
    "    \"random_scale_limit\": (-0.5, -0.2),\n",
    "    \"random_scale_prob\": 0.6,\n",
    "    \"pad_if_needed\": True,\n",
    "    \"pad_position\": \"random\",\n",
    "    \"pad_border_mode\": \"reflect\",\n",
    "    \n",
    "    \"hflip_prob\": 0.5,\n",
    "    \"rotation_degrees\": 10,\n",
    "    \"rotation_prob\": 0.4,\n",
    "    \"perspective_scale\": 0.1,\n",
    "    \"perspective_prob\": 0.2,\n",
    "    \n",
    "    \"brightness_jitter\": 0.2,\n",
    "    \"contrast_jitter\": 0.15,\n",
    "    \"saturation_jitter\": 0.2,\n",
    "    \"hue_jitter\": 0.03,\n",
    "    \"color_jitter_prob\": 0.7,\n",
    "    \"grayscale_prob\": 0.03,\n",
    "    \n",
    "    \"gaussian_blur_kernel\": (3, 5),\n",
    "    \"gaussian_blur_sigma\": (0.3, 1.2), \n",
    "    \"gaussian_blur_prob\": 0.3,\n",
    "    \"gaussian_noise_var\": (10.0, 50.0),\n",
    "    \"gaussian_noise_prob\": 0.25,\n",
    "\n",
    "    \"motion_blur_kernel_size\": (3, 5),\n",
    "    \"motion_blur_prob\": 0.15,\n",
    "    \n",
    "    \"jpeg_compression_quality\": (40, 90),\n",
    "    \"jpeg_compression_prob\": 0.3,\n",
    "    \n",
    "    \"coarse_dropout_max_holes\": 6,\n",
    "    \"coarse_dropout_max_height\": 30,\n",
    "    \"coarse_dropout_max_width\": 30,\n",
    "    \"coarse_dropout_prob\": 0.15,\n",
    "\n",
    "    \"normalize_mean\": [0.485, 0.456, 0.406],\n",
    "    \"normalize_std\": [0.229, 0.224, 0.225]\n",
    "}\n",
    "\n",
    "val_aug_args = {\n",
    "    \"imgsz\": 224,\n",
    "    \"normalize_mean\": [0.485, 0.456, 0.406],\n",
    "    \"normalize_std\": [0.229, 0.224, 0.225]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f021ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CatCamDataset(root_dir, train_aug_args)\n",
    "val_dataset = CatCamDataset(root_dir, val_aug_args, mode=\"val\")\n",
    "test_dataset = CatCamDataset(root_dir, val_aug_args, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f16bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81161443",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup model\n",
    "model = setup_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "val_metrics = evaluate(model, val_dataloader, cuda_device)\n",
    "show_metrics(val_metrics)\n",
    "sparsity = measure_global_sparsity(\n",
    "    model,\n",
    "    weight=True,\n",
    "    conv2d_use_mask=True,\n",
    "    linear_use_mask=True)\n",
    "print(f\"Sparsity: {sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea73182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train parameters\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "l1_reg_strength = 0\n",
    "l2_reg_strength = 1e-4\n",
    "lr = 1e-3\n",
    "lr_decay = 1\n",
    "early_stopping_patience = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=l2_reg_strength\n",
    "        )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_history = train_loop(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    cuda_device,\n",
    "    epochs,\n",
    "    early_stopping_patience\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1c75e",
   "metadata": {},
   "source": [
    "to visuzalize fine-tuning process in term of metrics & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ff815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving fine-tuned model\n",
    "save_model(model, model_dir, model_filename=model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e098e15",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prune parameters\n",
    "prune_lr = 1e-3\n",
    "prune_l1_reg_strength = 1e-3\n",
    "prune_l2_reg_strength = 1e-3\n",
    "conv2d_prune_amount = 0.7\n",
    "linear_prune_amount = 0.7\n",
    "num_iterations = 3\n",
    "num_epochs_per_iteration =  5\n",
    "grouped_pruning = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pruning\n",
    "print(\"Iterative pruning + Fine-Tuning...\")\n",
    "\n",
    "pruned_model = copy.deepcopy(model).to(cuda_device)\n",
    "\n",
    "prune_history = iterative_pruning_finetuning(\n",
    "        model=pruned_model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=test_dataloader,\n",
    "        device=cuda_device,\n",
    "        learning_rate=prune_lr,\n",
    "        l1_regularization_strength=prune_l1_reg_strength,\n",
    "        l2_regularization_strength=prune_l2_reg_strength,\n",
    "        conv2d_prune_amount=conv2d_prune_amount,\n",
    "        linear_prune_amount=linear_prune_amount,\n",
    "        num_iterations=num_iterations,\n",
    "        num_epochs_per_iteration=num_epochs_per_iteration,\n",
    "        model_dir=model_dir,\n",
    "        grouped_pruning=grouped_pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc7ea6",
   "metadata": {},
   "source": [
    "to visualize pruning process in term of metrics & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation after pruning\n",
    "val_metrics = evaluate(pruned_model, val_dataloader, cuda_device)\n",
    "show_metrics(val_metrics)\n",
    "sparsity = measure_global_sparsity(\n",
    "    pruned_model,\n",
    "    weight=True,\n",
    "    bias=False,\n",
    "    conv2d_use_mask=True,\n",
    "    linear_use_mask=True)#, conv2d_use_mask=True)\n",
    "print(f\"Sparsity: {sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ff2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = remove_parameters(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation after pruning\n",
    "val_metrics = evaluate(pruned_model, val_dataloader, cuda_device)\n",
    "show_metrics(val_metrics)\n",
    "sparsity = measure_global_sparsity(\n",
    "    pruned_model,\n",
    "    weight=True,\n",
    "    bias=False,\n",
    "    conv2d_use_mask=True,\n",
    "    linear_use_mask=True)#, conv2d_use_mask=True)\n",
    "print(f\"Sparsity: {sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving pruned model\n",
    "save_model(pruned_model, model_dir, model_filename=pruned_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447d231",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantization parameters\n",
    "quantization_lr = 1e-3\n",
    "quantization_wd = 4e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fusing model\n",
    "pruned_model.to(cpu_device)\n",
    "fused_model = fuse_model(model, model_name)\n",
    "\n",
    "pruned_model.eval()\n",
    "fused_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef60c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model quantization (QAT)\n",
    "quantized_model = QuantizedModel(model_fp32=fused_model.to(cuda_device))\n",
    "quantization_config = torch.quantization.get_default_qconfig(\"qnnpack\") #or fbgemm\n",
    "quantized_model.qconfig = quantization_config\n",
    "torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
    "\n",
    "#calibration\n",
    "with torch.no_grad():\n",
    "    for _ in range(15):\n",
    "        calibr = torch.rand((8, 3, 224, 224))\n",
    "        quantized_model(calibr.to(cuda_device))\n",
    "\n",
    "quantized_model.to(cpu_device).train()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "        quantized_model.parameters(),\n",
    "        lr=quantization_lr,\n",
    "        weight_decay=quantization_wd\n",
    "    )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "print(\"Training QAT Model...\")\n",
    "\n",
    "quantized_model.train()\n",
    "qat_history = train_loop(\n",
    "    quantized_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    cpu_device,\n",
    "    epochs,\n",
    "    early_stopping_patience) #to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e38169",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.eval().to(cpu_device)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        quantized_model,\n",
    "        dummy_input,\n",
    "        onnx_converted_model_filepath,\n",
    "        opset_version=13,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
    "        do_constant_folding=True,\n",
    "        training=torch.onnx.TrainingMode.EVAL,\n",
    "        keep_initializers_as_inputs=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f86f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only after onnx-convertation\n",
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving qat model\n",
    "save_model(quantized_model, model_dir, model_filename=quantized_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f685282",
   "metadata": {},
   "source": [
    "onnx2tf -i model_name.onnx -o model_tflite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemorr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
